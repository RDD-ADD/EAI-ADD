<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Emotion and Acoustics Should Agree: Cross-Level Inconsistency Analysis for Audio Deepfake Detection</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #f8fafc;
      color: #1e293b;
      margin: 0;
    }
    header {
      background: linear-gradient(90deg, #60a5fa, #38bdf8);
      color: white;
      text-align: center;
      padding: 2rem 1rem;
    }
    section {
      max-width: 1000px;
      margin: 2rem auto;
      padding: 1rem 2rem;
      background: white;
      border-radius: 10px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h2 {
      border-bottom: 2px solid #e2e8f0;
      padding-bottom: 0.5rem;
      color: #0f172a;
    }
    .abstract {
      background-color: #f1f5f9;
      padding: 1rem;
      border-radius: 8px;
    }
    .figure {
      text-align: center;
      margin: 1rem 0;
    }
    .figure img {
      max-width: 100%;
      border-radius: 8px;
      border: 1px solid #e2e8f0;
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 1rem;
    }
    th, td {
      border: 1px solid #e2e8f0;
      padding: 0.6rem;
      text-align: center;
    }
    th {
      background-color: #f1f5f9;
    }
    img.sample {
      width: 100%;
      border-radius: 8px;
    }
    .bonafide {
      background-color: #e0f2fe;
    }
    .spoof {
      background-color: #fee2e2;
    }
  </style>
</head>
<body>
  <header>
    <h1>Emotion and Acoustics Should Agree: Cross-Level Inconsistency Analysis for Audio Deepfake Detection</h1>
    <h3>EAI-ADD</h3>
  </header>

  <section>
    <h2>Abstract</h2>
    <div class="abstract">
      <p>
        Audio Deepfake Detection (ADD) aims to detect spoof speech from bonafide speech. Most prior studies assume that stronger correlations within or across acoustic and emotional features imply authenticity, and thus focus on enhancing or measuring such correlations.
        However, existing methods often treat acoustic and emotional features in isolation or rely on correlation metrics, which overlook subtle desynchronization between them and smooth out abrupt discontinuities.
        To address these issues, we propose EAI-ADD, which treats cross-level emotion–acoustic inconsistency as the primary detection signal. We first project emotional and acoustic representations into a comparable space. Then we progressively integrate frame- and utterance-level emotion features with acoustic features to capture cross-level emotion–acoustic inconsistencies across different temporal granularities. Experimental results on the ASVspoof 2019LA and 2021LA datasets demonstrate that the proposed EAI-ADD outperforms state-of-the-art baselines, providing a more effective solution for audio anti-spoofing detection.
      </p>
    </div>
  </section>

  <section>
    <h2>Model Architecture</h2>
    <div class="figure">
      <img src="1.png" alt="Model Architecture Diagram">
      <p><em>Figure:</em> Our overall framework is illustrated in (a). (b) illustrates the Emotion-Acoustic Alignment Module. (c) depicts the Emotion-Acoustic Inconsistency Modeling Module.</p>
    </div>
  </section>

  <!-- ======= Examples Section (with summary block inserted) ======= -->
  <section>
    <h2>More examples of Frame-level change magnitudes of emotion and acoustic features.</h2>

    <!-- Summary/Takeaway box placed right under the title -->
    <div class="abstract">
  <p>
     We compute the <strong>frame-level change magnitudes</strong> of emotional and acoustic features using the
    <strong>L2 distance</strong>, and apply <strong>min–max normalization</strong> to map them to [0,1], enabling
    comparisons on a common scale.
  </p>
  <ul>
    <li>
      <strong>Bonafide.</strong>
      The two curves exhibit <em>coherent co-variation</em> over extended intervals; peaks and troughs are generally
      aligned within a <em>small and stable temporal lag</em>, indicating <em>synchronization</em> between acoustic
      fluctuations and emotional dynamics.
    </li>
    <li>
      <strong>Spoof.</strong>
      Clear <em>desynchronization</em> is observed: sharp peaks in the emotion curve often
      <em>lack corresponding acoustic responses</em> (and vice versa), accompanied by larger and unstable
      <em>phase offsets</em>, <em>unilateral dominance</em>, and patterns characterized by
      <em>extended plateau segments interspersed with impulsive bursts</em>.
    </li>
    <li>
      <strong>Takeaway.</strong>
      Rather than using correlation as a surrogate for authenticity—which can smooth over fine-grained misalignments—we
      <strong>explicitly model emotion–acoustic mismatches</strong> as direct and discriminative cues, under the premise
      that such mismatches are <em>rare in bonafide speech</em> but <em>prevalent in spoofed audio</em>.
    </li>
  </ul>
</div>


    <div class="bonafide">
      <table>
        <tbody>
          <tr><td><img class="sample" src="zx1.png" alt="Sample 1"></td></tr>
          <tr><td><img class="sample" src="zx2.png" alt="Sample 2"></td></tr>
          <tr><td><img class="sample" src="zx3.png" alt="Sample 3"></td></tr>
          <tr><td><img class="sample" src="zx4.png" alt="Sample 4"></td></tr>
          <tr><td><img class="sample" src="zx5.png" alt="Sample 5"></td></tr>
        </tbody>
      </table>
    </div>
  </section>
</body>
</html>
